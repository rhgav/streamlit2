import io
import streamlit as st
from PIL import Image
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# è®¾ç½®page_titleå†…å®¹
st.set_page_config(page_title="é—®ç­”æœºå™¨äºº")

# è®¾ç½®é¦–è¡Œå†…å®¹
st.title('ğŸ¤–é—®ç­”æœºå™¨äººğŸ˜œ')

# åˆå§‹åŒ–èŠå¤©è®°å½•å’Œè®°å¿†
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.memory = ConversationBufferMemory(memory_key='chat_history')

# å±•ç¤ºèŠå¤©è®°å½•
for message in st.session_state.messages:
    if message["role"] == "user":
        with st.chat_message(message["role"], avatar='â˜ºï¸'):
            st.markdown(message["content"])
    else:
        with st.chat_message(message["role"], avatar='ğŸ¤–'):
            st.markdown(message["content"])

def generate_response(input_text):
    """
    :param input_text:  ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢é—®é¢˜ï¼Œç±»å‹ä¸ºstr
    :return: è¿”å›langchainæŸ¥è¯¢openaiè·å¾—çš„ç»“æœï¼Œç±»å‹ä¸ºstr
    """
    llm = ChatOpenAI(
        temperature=0.95,
        model="glm-4-plus",
        openai_api_key="3e28ff391d064016b4e95f3e3b792d82.IGQKeHaJqo7Lxw2E",
        openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
    )

    template = """
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„ä»£ç ä¿®æ”¹å’ŒèŠå¤©æœºå™¨äººã€‚
    ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. å¦‚æœç”¨æˆ·æäº¤çš„æ˜¯ä»£ç ç‰‡æ®µï¼Œä½ åº”åˆ†æä»£ç ï¼Œæä¾›ä¿®æ”¹å»ºè®®ï¼ˆå¦‚æœå¿…è¦ï¼‰ï¼Œæˆ–è§£é‡Šä»£ç çš„åŠŸèƒ½å’Œå·¥ä½œåŸç†ã€‚
    2. å¦‚æœç”¨æˆ·æäº¤çš„æ˜¯éä»£ç æ–‡æœ¬ï¼Œä½ åº”ä¸ä¹‹è¿›è¡Œå¯¹è¯ï¼Œä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ã€‚

    æ³¨æ„ï¼š
    - åœ¨æä¾›ä»£ç ä¿®æ”¹å»ºè®®æ—¶ï¼Œè¯·ç¡®ä¿ä½ çš„å»ºè®®æ˜¯å¯æ“ä½œçš„ï¼Œå¹¶ä¸”å°½é‡ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€ã€‚
    - å¯¹äºéä»£ç æ–‡æœ¬ï¼Œå°½é‡ä¿æŒå¯¹è¯çš„ç›¸å…³æ€§å’Œæœ‰è¶£æ€§ã€‚
    - å¦‚æœæ— æ³•æä¾›æœ‰ç”¨çš„ä¿¡æ¯æˆ–æ— æ³•å¤„ç†ç”¨æˆ·çš„é—®é¢˜ï¼Œå¯ä»¥å›å¤ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æˆ–å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚â€

    ä»¥ä¸‹æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š
    {question}
    """
    prompt = PromptTemplate(template=template, input_variables=["question"])

    output_parser = StrOutputParser()
    chain = prompt | llm | output_parser

    # å°†å¯¹è¯å†å²ä¼ é€’ç»™æ¨¡å‹
    response = chain.invoke({"question": input_text, "chat_history": st.session_state.memory.load_memory_variables({})})
    st.session_state.memory.save_context({"input": input_text}, {"output": response})
    return response

# ç”¨äºç”¨æˆ·è¾“å…¥
if prompt := st.chat_input('æˆ‘ä»¬æ¥èŠä¸€ç‚¹ä»£ç ç›¸å…³çš„äº‹å„¿å§'):
    with st.chat_message('user', avatar='â˜ºï¸'):
        st.markdown(prompt)

    st.session_state.messages.append({'role': 'user', 'content': prompt})

    with st.spinner("AIæ­£åœ¨é£å¿«åŠ è½½ä¸­..."):
        response = generate_response(prompt)

    with st.chat_message('assistant', avatar='ğŸ¤–'):
        st.markdown(response)
    st.session_state.messages.append({'role': 'assistant', 'content': response})